{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Auto reload so I don't need to restart the kernel each time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:47:01.860790Z",
     "start_time": "2024-03-31T11:47:01.826110Z"
    }
   },
   "id": "5925b8fe4868cd38",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95837702-6c7f-440f-8c77-4c259a7a91a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T11:47:05.071771Z",
     "start_time": "2024-03-31T11:47:02.419471Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate ID3 decision tree\n",
    "# Required: Don't use scikit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import hashlib\n",
    "import networkx as nx\n",
    "from treelib import Node, Tree\n",
    "from contextlib import redirect_stdout\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "#from code.plotting import plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97aab7d8-e597-45e9-99f7-d5b49b870bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-31T11:47:43.062679Z",
     "start_time": "2024-03-31T11:47:42.993437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_csv_dataframe: \n",
      "    PassengerId  Pclass     Sex     Age  SibSp  Parch  Survived\n",
      "0            1       3    male  middle      1      0         0\n",
      "1            2       1  female  middle      1      0         1\n",
      "2            3       3  female  middle      0      0         1\n",
      "3            4       1  female  middle      1      0         1\n",
      "4            5       3    male  middle      0      0         0\n"
     ]
    }
   ],
   "source": [
    "## Import data\n",
    "FILE = '../data/put-titanic-homework.csv'\n",
    "columns_to_omit = [\n",
    "    'Name']  # Omit name for simplicity, we assume it doesn't affect the chances of survival (it musn't be the case! Our assumption)\n",
    "input_csv_dataframe = pd.read_csv(FILE, usecols=lambda column: column not in columns_to_omit)\n",
    "## PREPROCESS\n",
    "# Map age to one of 3 categories\n",
    "# Age[0,20] = young || Age[20,40] = middle || Age[40,99999] = old\n",
    "input_csv_dataframe['Age'] = input_csv_dataframe['Age'].map(\n",
    "    lambda x: 'young' if 0 <= x <= 20 else ('middle' if 20 < x <= 40 else 'old'))\n",
    "print(f\"input_csv_dataframe: \\n {input_csv_dataframe.head(n=5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete whole_dataset_entropy: 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "# Calculating the entropy of the whole dataset\n",
    "def calc_value_entropy(feature, value_key, df):\n",
    "    survived_sum_by_value = df.loc[df[feature] == value_key, 'Survived'].sum()\n",
    "    p_survived = survived_sum_by_value / df.shape[0]\n",
    "    p_no_survived = 1 - p_survived\n",
    "    entropy = (- p_no_survived * np.log2(p_no_survived, where=(p_no_survived != 0)) -\n",
    "               p_survived * np.log2(p_survived, where=(p_survived != 0)))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "whole_dataset_entropy = calc_value_entropy('Survived', 1, input_csv_dataframe)\n",
    "print(f\"Complete whole_dataset_entropy: {whole_dataset_entropy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:47:43.498577Z",
     "start_time": "2024-03-31T11:47:43.359368Z"
    }
   },
   "id": "895e6a05911a6124",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Feature': 'Pclass', 'Value': 1, 'P': 0.21, 'Cases': [2, 4, 7, 12, 24, 28, 31, 32, 35, 36, 53, 55, 56, 62, 63, 65, 84, 89, 93, 97, 98], 'Entropy': 0.5293608652873644}\n",
      "{'Feature': 'Pclass', 'Value': 2, 'P': 0.19, 'Cases': [10, 16, 18, 21, 22, 34, 42, 44, 54, 57, 59, 67, 71, 73, 79, 82, 85, 99, 100], 'Entropy': 0.5293608652873644}\n",
      "{'Feature': 'Pclass', 'Value': 3, 'P': 0.6, 'Cases': [1, 3, 5, 6, 8, 9, 11, 13, 14, 15, 17, 19, 20, 23, 25, 26, 27, 29, 30, 33, 37, 38, 39, 40, 41, 43, 45, 46, 47, 48, 49, 50, 51, 52, 58, 60, 61, 64, 66, 68, 69, 70, 72, 74, 75, 76, 77, 78, 80, 81, 83, 86, 87, 88, 90, 91, 92, 94, 95, 96], 'Entropy': 0.6343095546405662}\n",
      "{'Feature': 'Sex', 'Value': 'male', 'P': 0.6, 'Cases': [1, 5, 6, 7, 8, 13, 14, 17, 18, 21, 22, 24, 27, 28, 30, 31, 34, 35, 36, 37, 38, 43, 46, 47, 49, 51, 52, 55, 56, 58, 60, 61, 63, 64, 65, 66, 68, 70, 71, 73, 74, 75, 76, 77, 78, 79, 81, 82, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100], 'Entropy': 0.36592365090022333}\n",
      "{'Feature': 'Sex', 'Value': 'female', 'P': 0.4, 'Cases': [2, 3, 4, 9, 10, 11, 12, 15, 16, 19, 20, 23, 25, 26, 29, 32, 33, 39, 40, 41, 42, 44, 45, 48, 50, 53, 54, 57, 59, 62, 67, 69, 72, 80, 83, 84, 85, 86, 89, 99], 'Entropy': 0.9149263727797275}\n",
      "{'Feature': 'Age', 'Value': 'young', 'P': 0.29, 'Cases': [8, 10, 11, 13, 15, 17, 18, 23, 25, 28, 29, 39, 40, 44, 45, 48, 50, 51, 56, 59, 60, 64, 68, 69, 72, 79, 85, 87, 92], 'Entropy': 0.584238811642856}\n",
      "{'Feature': 'Age', 'Value': 'old', 'P': 0.21, 'Cases': [7, 12, 16, 20, 27, 30, 32, 33, 34, 36, 47, 49, 53, 55, 63, 65, 83, 93, 95, 96, 97], 'Entropy': 0.36592365090022333}\n",
      "{'Feature': 'Age', 'Value': 'middle', 'P': 0.5, 'Cases': [1, 2, 3, 4, 5, 6, 9, 14, 19, 21, 22, 24, 26, 31, 35, 37, 38, 41, 42, 43, 46, 52, 54, 57, 58, 61, 62, 66, 67, 70, 71, 73, 74, 75, 76, 77, 78, 80, 81, 82, 84, 86, 88, 89, 90, 91, 94, 98, 99, 100], 'Entropy': 0.7014714598838974}\n",
      "{'Feature': 'SibSp', 'Value': 0, 'P': 0.59, 'Cases': [3, 5, 6, 7, 9, 12, 13, 15, 16, 18, 20, 21, 22, 23, 24, 27, 29, 30, 31, 33, 34, 37, 38, 43, 45, 46, 48, 52, 55, 56, 57, 58, 61, 62, 65, 67, 68, 71, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 88, 90, 91, 92, 95, 96, 97, 98, 99], 'Entropy': 0.8112781244591328}\n",
      "{'Feature': 'SibSp', 'Value': 1, 'P': 0.27, 'Cases': [1, 2, 4, 10, 11, 14, 19, 26, 32, 35, 36, 40, 41, 42, 44, 47, 50, 53, 54, 59, 63, 66, 74, 87, 93, 94, 100], 'Entropy': 0.5293608652873644}\n",
      "{'Feature': 'SibSp', 'Value': 2, 'P': 0.03, 'Cases': [39, 49, 70], 'Entropy': -0.0}\n",
      "{'Feature': 'SibSp', 'Value': 3, 'P': 0.06, 'Cases': [8, 25, 28, 64, 86, 89], 'Entropy': 0.14144054254182067}\n",
      "{'Feature': 'SibSp', 'Value': 4, 'P': 0.03, 'Cases': [17, 51, 69], 'Entropy': 0.08079313589591118}\n",
      "{'Feature': 'SibSp', 'Value': 5, 'P': 0.02, 'Cases': [60, 72], 'Entropy': -0.0}\n",
      "{'Feature': 'Parch', 'Value': 0, 'P': 0.77, 'Cases': [1, 2, 3, 4, 5, 6, 7, 10, 12, 13, 15, 16, 18, 19, 20, 21, 22, 23, 24, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 52, 53, 54, 56, 57, 58, 61, 62, 63, 65, 67, 68, 70, 71, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 90, 91, 92, 93, 95, 96, 97, 100], 'Entropy': 0.8687212463394045}\n",
      "{'Feature': 'Parch', 'Value': 1, 'P': 0.09, 'Cases': [8, 11, 17, 25, 51, 55, 66, 98, 99], 'Entropy': 0.24229218908241482}\n",
      "{'Feature': 'Parch', 'Value': 2, 'P': 0.11, 'Cases': [9, 28, 44, 59, 60, 64, 69, 72, 79, 89, 94], 'Entropy': 0.32744491915447627}\n",
      "{'Feature': 'Parch', 'Value': 3, 'P': 0.01, 'Cases': [87], 'Entropy': -0.0}\n",
      "{'Feature': 'Parch', 'Value': 5, 'P': 0.02, 'Cases': [14, 26], 'Entropy': 0.08079313589591118}\n"
     ]
    }
   ],
   "source": [
    "# Calculating the entropy for the filtered dataset\n",
    "features_data_list = []\n",
    "for column in input_csv_dataframe.columns:\n",
    "\n",
    "    if column == 'Survived' or column == 'PassengerId':\n",
    "        continue  # because these are not features\n",
    "\n",
    "    # Get all unique values from column\n",
    "    values_set = set(input_csv_dataframe[column].unique())\n",
    "\n",
    "    for value in values_set:\n",
    "        features_dict = {}\n",
    "        features_dict['Feature'] = column\n",
    "        features_dict['Value'] = value\n",
    "        features_dict['P'] = input_csv_dataframe[column].eq(value).sum() / len(input_csv_dataframe)\n",
    "        features_dict['Cases'] = input_csv_dataframe.loc[input_csv_dataframe[column] == value, 'PassengerId'].to_list()\n",
    "        features_dict['Entropy'] = calc_value_entropy(column, value, input_csv_dataframe)\n",
    "        features_data_list.append(features_dict)\n",
    "\n",
    "print(*features_data_list, sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:47:43.933120Z",
     "start_time": "2024-03-31T11:47:43.819320Z"
    }
   },
   "id": "e50a02aa89a6b33e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_data_dataframe: \n",
      "   Feature Value     P                                              Cases  \\\n",
      "0  Pclass     1  0.21  [2, 4, 7, 12, 24, 28, 31, 32, 35, 36, 53, 55, ...   \n",
      "1  Pclass     2  0.19  [10, 16, 18, 21, 22, 34, 42, 44, 54, 57, 59, 6...   \n",
      "2  Pclass     3  0.60  [1, 3, 5, 6, 8, 9, 11, 13, 14, 15, 17, 19, 20,...   \n",
      "3     Sex  male  0.60  [1, 5, 6, 7, 8, 13, 14, 17, 18, 21, 22, 24, 27...   \n",
      "\n",
      "    Entropy  \n",
      "0  0.529361  \n",
      "1  0.529361  \n",
      "2  0.634310  \n",
      "3  0.365924  \n"
     ]
    }
   ],
   "source": [
    "# Save calculated [ Feature, Value, P, Cases, Entropy ] to \"features_entropy.csv\"\n",
    "features_entropy_df = pd.DataFrame(features_data_list)\n",
    "features_entropy_df.to_csv(\"../data/calculated_features_entropy.csv\", sep=',', index=False, encoding='utf-8')\n",
    "print(f\"features_data_dataframe: \\n {features_entropy_df.head(n=4)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:48:05.730707Z",
     "start_time": "2024-03-31T11:48:05.668426Z"
    }
   },
   "id": "dde1874557b68eb7",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature   Value     P                                              Cases  \\\n",
      "3     Sex    male  0.60  [1, 5, 6, 7, 8, 13, 14, 17, 18, 21, 22, 24, 27...   \n",
      "4     Sex  female  0.40  [2, 3, 4, 9, 10, 11, 12, 15, 16, 19, 20, 23, 2...   \n",
      "0  Pclass       1  0.21  [2, 4, 7, 12, 24, 28, 31, 32, 35, 36, 53, 55, ...   \n",
      "2  Pclass       3  0.60  [1, 3, 5, 6, 8, 9, 11, 13, 14, 15, 17, 19, 20,...   \n",
      "\n",
      "    Entropy  InfoGain  InfoSplit  \n",
      "3  0.365924  0.385426   0.000000  \n",
      "4  0.914926  0.385426   0.000000  \n",
      "0  0.529361  0.378621   0.472823  \n",
      "2  0.634310  0.378621   0.472823  \n"
     ]
    }
   ],
   "source": [
    "# Calculate information gain and information split gain\n",
    "def calc_conditional_entropy(feature, df):\n",
    "    return (df.loc[df['Feature'] == feature, 'P'] * df.loc[df['Feature'] == feature, 'Entropy']).sum()\n",
    "\n",
    "\n",
    "def calc_split_gain(feature, df):\n",
    "    split_gain = 0\n",
    "    unique_values = set(df[column].unique())\n",
    "    for value in unique_values:\n",
    "        p = df[(df[feature] == value)].shape[0] / df.shape[0]\n",
    "        split_gain -= p * np.log2(p, where=(p != 0))\n",
    "    return split_gain\n",
    "\n",
    "\n",
    "features_by_information_gains = []\n",
    "for df_column in input_csv_dataframe.columns:\n",
    "\n",
    "    if df_column == 'Survived' or df_column == 'PassengerId':\n",
    "        continue  # because these are not features\n",
    "\n",
    "    info_gain = whole_dataset_entropy - calc_conditional_entropy(df_column, features_entropy_df)\n",
    "    info_split_gain = calc_split_gain(df_column, input_csv_dataframe)\n",
    "    features_entropy_df.loc[features_entropy_df[\"Feature\"] == df_column, \"InfoGain\"] = info_gain\n",
    "    features_entropy_df.loc[features_entropy_df[\"Feature\"] == df_column, \"InfoSplit\"] = info_split_gain\n",
    "\n",
    "feat_df_sort_by_gain = features_entropy_df.sort_values(by='InfoGain', ascending=False)\n",
    "print(feat_df_sort_by_gain.head(n=4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:47:47.784563Z",
     "start_time": "2024-03-31T11:47:47.684420Z"
    }
   },
   "id": "c35434a906330f55",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "b'Sex\\n'\n"
     ]
    }
   ],
   "source": [
    "# Build tree based on features_entropy_df\n",
    "def ID3(df, tree):\n",
    "    if tree.size() == 0:  # if tree empty\n",
    "        max_gain_feature_index = df[\"InfoGain\"].idxmax()\n",
    "        root = tree.create_node(df.loc[max_gain_feature_index, \"Feature\"])\n",
    "        \n",
    "        return\n",
    "    else:\n",
    "        best_feature = df.loc[df['InfoGain'].idxmax()]\n",
    "    #     root = Node(best_feature['Feature'])\n",
    "    #     for value in best_feature['Value']:\n",
    "    #         df_v = df.loc[df[best_feature['Feature']] == value]\n",
    "    #         child = ID3(df_v)\n",
    "    #         root.add_child(value, child)\n",
    "    #     return root\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "print(ID3(feat_df_sort_by_gain, tree))\n",
    "tree.show()\n",
    "\n",
    "\n",
    "# def ID3(D, A):\n",
    "#     if D.empty or A.empty:\n",
    "#         return D['Survived'].mode().iloc[0]\n",
    "#     else:\n",
    "#         A_best = A.loc[A['InfoGain'].idxmax()]\n",
    "#         root = Node(A_best['Feature'])\n",
    "#         for v in A_best['Value']:\n",
    "#             D_v = D.loc[D[A_best['Feature']] == v]\n",
    "#             child = ID3(D_v, A.drop(A_best))\n",
    "#             root.add_child(v, child)\n",
    "#         return root\n",
    "# \n",
    "# \n",
    "# def ID3(D, A):\n",
    "#   wenn D rein oder A leer ist:\n",
    "#     Gibt einen Blattknoten mit der Mehrheitsklasse in D zurück\n",
    "#   anders:\n",
    "#     A_best = argmax(InformationGain(D, A))\n",
    "#     root = Node(A_best)\n",
    "#     für v in Werten (A_best):\n",
    "#       D_v = Teilmenge(D, A_best, v)\n",
    "#       child = ID3(D_v, A - {A_best})\n",
    "#       root.add_child(v, child)\n",
    "#     Rückkehr zur Wurzel\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T20:42:50.355968Z",
     "start_time": "2024-03-30T20:42:50.291890Z"
    }
   },
   "id": "50310f93ea0c01d",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'False: boolean label can not be used without a boolean index'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m feature_df \u001B[38;5;241m=\u001B[39m \u001B[43mfeatures_entropy_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFeature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mfeature\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m      9\u001B[0m values \u001B[38;5;241m=\u001B[39m features_entropy_df\u001B[38;5;241m.\u001B[39mloc[features_entropy_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m feature[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValue\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_list()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m value \u001B[38;5;129;01min\u001B[39;00m values:\n",
      "File \u001B[0;32m~/src/github.com/titanic-machine-learning/venv/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1189\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[1;32m   1190\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001B[0;32m-> 1191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/src/github.com/titanic-machine-learning/venv/lib/python3.12/site-packages/pandas/core/indexing.py:1430\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1427\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;28mtuple\u001B[39m(indexer)]\n\u001B[1;32m   1429\u001B[0m \u001B[38;5;66;03m# fall thru to straight lookup\u001B[39;00m\n\u001B[0;32m-> 1430\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1431\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_label(key, axis\u001B[38;5;241m=\u001B[39maxis)\n",
      "File \u001B[0;32m~/src/github.com/titanic-machine-learning/venv/lib/python3.12/site-packages/pandas/core/indexing.py:1239\u001B[0m, in \u001B[0;36m_LocIndexer._validate_key\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1232\u001B[0m ax \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_get_axis(axis)\n\u001B[1;32m   1233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\n\u001B[1;32m   1234\u001B[0m     is_bool_dtype(ax\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m   1235\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m ax\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboolean\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1236\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ax, MultiIndex)\n\u001B[1;32m   1237\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_bool_dtype(ax\u001B[38;5;241m.\u001B[39mget_level_values(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m   1238\u001B[0m ):\n\u001B[0;32m-> 1239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[1;32m   1240\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: boolean label can not be used without a boolean index\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1241\u001B[0m     )\n\u001B[1;32m   1243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m   1244\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(key\u001B[38;5;241m.\u001B[39mstart, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key\u001B[38;5;241m.\u001B[39mstop, \u001B[38;5;28mbool\u001B[39m)\n\u001B[1;32m   1245\u001B[0m ):\n\u001B[1;32m   1246\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: boolean values can not be used in a slice\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'False: boolean label can not be used without a boolean index'"
     ]
    }
   ],
   "source": [
    "# Build tree basing on features_entropy_df\n",
    "\n",
    "\n",
    "## VISUALIZE\n",
    "# Create json\n",
    "tree = Tree()\n",
    "tree.create_node(features_by_information_gains[0][0], 0)\n",
    "for index, feature in enumerate(features_by_information_gains):\n",
    "    if index == 0:\n",
    "        continue\n",
    "    feature_df = features_entropy_df.loc[[\"Feature\"] == feature[0]]\n",
    "    values = features_entropy_df.loc[features_entropy_df[\"Feature\"] == feature[0], 'Value'].to_list()\n",
    "    for value in values:\n",
    "        hashy = int(hashlib.sha256(str(value).encode('utf-8')).hexdigest(), 16)\n",
    "\n",
    "        # if all examples were survivors make \"survived\" node\n",
    "        # if all dies make \"died\" node\n",
    "        # if neither \n",
    "        tree.create_node(features_by_information_gains[index][0], hashy, parent=index - 1, data=value)\n",
    "\n",
    "with open('tree.json', 'w') as f:\n",
    "    print(tree.to_json(with_data=True), file=f)\n",
    "\n",
    "# Print decision tree using generated json\n",
    "# 4) Draw decision tree (use package) with info how many\n",
    "data = json.loads(tree.to_json())\n",
    "edges = []\n",
    "\n",
    "\n",
    "def get_edges(treedict, parent=None):\n",
    "    name = next(iter(treedict.keys()))\n",
    "    if parent is not None:\n",
    "        edges.append((parent, name))\n",
    "    for item in treedict[name][\"children\"]:\n",
    "        if isinstance(item, dict):\n",
    "            get_edges(item, parent=name)\n",
    "        else:\n",
    "            edges.append((name, item))\n",
    "\n",
    "\n",
    "get_edges(data)\n",
    "\n",
    "# Dump edge list in Graphviz DOT format\n",
    "with open('tree.dot', 'w') as f:\n",
    "    print('strict digraph tree {', file=f)\n",
    "    for row in edges:\n",
    "        print('    {0} -> {1} [ label=\"value\" ];'.format(*row), file=f)\n",
    "    print('}', file=f)\n",
    "\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# for index, feature in enumerate(features_by_gains):\n",
    "#     G.add_node(features_by_gains[index][0])\n",
    "#     feature_possible_choices = set(features_entropy_df.loc[features_entropy_df[\"Feature\"] == features_by_gains[index][0], \"Value\"])\n",
    "#     G.add_nodes_from(feature_possible_choices)\n",
    "#     for choice in feature_possible_choices:\n",
    "#         G.add_edge(features_by_gains[index][0], choice)\n",
    "# \n",
    "# pos = graphviz_layout(<G, prog=\"dot\")\n",
    "# nx.draw(G, pos, with_labels=True, font_weight='bold')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:38:16.298247Z",
     "start_time": "2024-03-23T15:38:13.296619Z"
    }
   },
   "id": "53396ea6de62e88",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "start by writing function "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cdbf6fd25b5bacb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
