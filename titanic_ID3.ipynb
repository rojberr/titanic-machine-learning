{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95837702-6c7f-440f-8c77-4c259a7a91a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T16:52:59.077550Z",
     "start_time": "2024-03-30T16:52:59.000505Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate ID3 decision tree\n",
    "# Required: Don't use scikit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import hashlib\n",
    "import networkx as nx\n",
    "from treelib import Node, Tree\n",
    "from contextlib import redirect_stdout\n",
    "from networkx.drawing.nx_pydot import graphviz_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97aab7d8-e597-45e9-99f7-d5b49b870bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T16:52:59.770217Z",
     "start_time": "2024-03-30T16:52:59.685936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_csv_dataframe: \n",
      "    PassengerId  Pclass     Sex     Age  SibSp  Parch  Survived\n",
      "0            1       3    male  middle      1      0         0\n",
      "1            2       1  female  middle      1      0         1\n",
      "2            3       3  female  middle      0      0         1\n",
      "3            4       1  female  middle      1      0         1\n",
      "4            5       3    male  middle      0      0         0\n"
     ]
    }
   ],
   "source": [
    "## Import data\n",
    "FILE = 'titanic-homework.csv'\n",
    "columns_to_omit = [\n",
    "    'Name']  # Omit name for simplicity, we assume it doesn't affect the chances of survival (it musn't be the case! Our assumption)\n",
    "input_csv_dataframe = pd.read_csv(FILE, usecols=lambda column: column not in columns_to_omit)\n",
    "## PREPROCESS\n",
    "# Map age to one of 3 categories\n",
    "# Age[0,20] = young || Age[20,40] = middle || Age[40,99999] = old\n",
    "input_csv_dataframe['Age'] = input_csv_dataframe['Age'].map(\n",
    "    lambda x: 'young' if 0 <= x <= 20 else ('middle' if 20 < x <= 40 else 'old'))\n",
    "print(f\"input_csv_dataframe: \\n {input_csv_dataframe.head(n=5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete whole_dataset_entropy: 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "# Calculating the entropy of the whole dataset\n",
    "def calc_value_entropy(feature, value_key, df):\n",
    "    survived_sum_by_value = df.loc[df[feature] == value_key, 'Survived'].sum()\n",
    "    p_survived = survived_sum_by_value / df.shape[0]\n",
    "    p_no_survived = 1 - p_survived\n",
    "    entropy = (- p_no_survived * np.log2(p_no_survived, where=(p_no_survived != 0)) -\n",
    "               p_survived * np.log2(p_survived, where=(p_survived != 0)))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "whole_dataset_entropy = calc_value_entropy('Survived', 1, input_csv_dataframe)\n",
    "print(f\"Complete whole_dataset_entropy: {whole_dataset_entropy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T16:53:00.903992Z",
     "start_time": "2024-03-30T16:53:00.392311Z"
    }
   },
   "id": "895e6a05911a6124",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pclass', 1, 0.21, [2, 4, 7, 12, 24, 28, 31, 32, 35, 36, 53, 55, 56, 62, 63, 65, 84, 89, 93, 97, 98], 0.5293608652873644]\n",
      "['Pclass', 2, 0.19, [10, 16, 18, 21, 22, 34, 42, 44, 54, 57, 59, 67, 71, 73, 79, 82, 85, 99, 100], 0.5293608652873644]\n",
      "['Pclass', 3, 0.6, [1, 3, 5, 6, 8, 9, 11, 13, 14, 15, 17, 19, 20, 23, 25, 26, 27, 29, 30, 33, 37, 38, 39, 40, 41, 43, 45, 46, 47, 48, 49, 50, 51, 52, 58, 60, 61, 64, 66, 68, 69, 70, 72, 74, 75, 76, 77, 78, 80, 81, 83, 86, 87, 88, 90, 91, 92, 94, 95, 96], 0.6343095546405662]\n",
      "['Sex', 'male', 0.6, [1, 5, 6, 7, 8, 13, 14, 17, 18, 21, 22, 24, 27, 28, 30, 31, 34, 35, 36, 37, 38, 43, 46, 47, 49, 51, 52, 55, 56, 58, 60, 61, 63, 64, 65, 66, 68, 70, 71, 73, 74, 75, 76, 77, 78, 79, 81, 82, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100], 0.36592365090022333]\n",
      "['Sex', 'female', 0.4, [2, 3, 4, 9, 10, 11, 12, 15, 16, 19, 20, 23, 25, 26, 29, 32, 33, 39, 40, 41, 42, 44, 45, 48, 50, 53, 54, 57, 59, 62, 67, 69, 72, 80, 83, 84, 85, 86, 89, 99], 0.9149263727797275]\n",
      "['Age', 'middle', 0.5, [1, 2, 3, 4, 5, 6, 9, 14, 19, 21, 22, 24, 26, 31, 35, 37, 38, 41, 42, 43, 46, 52, 54, 57, 58, 61, 62, 66, 67, 70, 71, 73, 74, 75, 76, 77, 78, 80, 81, 82, 84, 86, 88, 89, 90, 91, 94, 98, 99, 100], 0.7014714598838974]\n",
      "['Age', 'young', 0.29, [8, 10, 11, 13, 15, 17, 18, 23, 25, 28, 29, 39, 40, 44, 45, 48, 50, 51, 56, 59, 60, 64, 68, 69, 72, 79, 85, 87, 92], 0.584238811642856]\n",
      "['Age', 'old', 0.21, [7, 12, 16, 20, 27, 30, 32, 33, 34, 36, 47, 49, 53, 55, 63, 65, 83, 93, 95, 96, 97], 0.36592365090022333]\n",
      "['SibSp', 0, 0.59, [3, 5, 6, 7, 9, 12, 13, 15, 16, 18, 20, 21, 22, 23, 24, 27, 29, 30, 31, 33, 34, 37, 38, 43, 45, 46, 48, 52, 55, 56, 57, 58, 61, 62, 65, 67, 68, 71, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 88, 90, 91, 92, 95, 96, 97, 98, 99], 0.8112781244591328]\n",
      "['SibSp', 1, 0.27, [1, 2, 4, 10, 11, 14, 19, 26, 32, 35, 36, 40, 41, 42, 44, 47, 50, 53, 54, 59, 63, 66, 74, 87, 93, 94, 100], 0.5293608652873644]\n",
      "['SibSp', 2, 0.03, [39, 49, 70], -0.0]\n",
      "['SibSp', 3, 0.06, [8, 25, 28, 64, 86, 89], 0.14144054254182067]\n",
      "['SibSp', 4, 0.03, [17, 51, 69], 0.08079313589591118]\n",
      "['SibSp', 5, 0.02, [60, 72], -0.0]\n",
      "['Parch', 0, 0.77, [1, 2, 3, 4, 5, 6, 7, 10, 12, 13, 15, 16, 18, 19, 20, 21, 22, 23, 24, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 52, 53, 54, 56, 57, 58, 61, 62, 63, 65, 67, 68, 70, 71, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 90, 91, 92, 93, 95, 96, 97, 100], 0.8687212463394045]\n",
      "['Parch', 1, 0.09, [8, 11, 17, 25, 51, 55, 66, 98, 99], 0.24229218908241482]\n",
      "['Parch', 2, 0.11, [9, 28, 44, 59, 60, 64, 69, 72, 79, 89, 94], 0.32744491915447627]\n",
      "['Parch', 3, 0.01, [87], -0.0]\n",
      "['Parch', 5, 0.02, [14, 26], 0.08079313589591118]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the entropy for the filtered dataset\n",
    "features_entropy_list = []\n",
    "for column in input_csv_dataframe.columns:\n",
    "\n",
    "    if column == 'Survived' or column == 'PassengerId':\n",
    "        continue\n",
    "\n",
    "    # Get all unique values from column\n",
    "    values_set = set(input_csv_dataframe[column].unique())\n",
    "\n",
    "    # calc P of each decision\n",
    "    #conditional_entropy = 0\n",
    "    for value in values_set:\n",
    "        value_p = input_csv_dataframe[column].eq(value).sum() / len(input_csv_dataframe)\n",
    "        cases = input_csv_dataframe.loc[input_csv_dataframe[column] == value, 'PassengerId'].to_list()\n",
    "        value_entropy = calc_value_entropy(column, value, input_csv_dataframe)\n",
    "        features_entropy_list.append([column, value, value_p, cases, value_entropy])\n",
    "        \n",
    "print(*features_entropy_list, sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T16:56:12.961324Z",
     "start_time": "2024-03-30T16:56:12.911717Z"
    }
   },
   "id": "e50a02aa89a6b33e",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feea6f54-c306-4cb4-9e79-896909081c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T16:24:25.073482Z",
     "start_time": "2024-03-30T16:24:24.920708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature   Value     P                                              Cases  \\\n",
      "0   Pclass       1  0.21  [2, 4, 7, 12, 24, 28, 31, 32, 35, 36, 53, 55, ...   \n",
      "1   Pclass       2  0.19  [10, 16, 18, 21, 22, 34, 42, 44, 54, 57, 59, 6...   \n",
      "2   Pclass       3  0.60  [1, 3, 5, 6, 8, 9, 11, 13, 14, 15, 17, 19, 20,...   \n",
      "3      Sex    male  0.60  [1, 5, 6, 7, 8, 13, 14, 17, 18, 21, 22, 24, 27...   \n",
      "4      Sex  female  0.40  [2, 3, 4, 9, 10, 11, 12, 15, 16, 19, 20, 23, 2...   \n",
      "5      Age  middle  0.50  [1, 2, 3, 4, 5, 6, 9, 14, 19, 21, 22, 24, 26, ...   \n",
      "6      Age   young  0.29  [8, 10, 11, 13, 15, 17, 18, 23, 25, 28, 29, 39...   \n",
      "7      Age     old  0.21  [7, 12, 16, 20, 27, 30, 32, 33, 34, 36, 47, 49...   \n",
      "8    SibSp       0  0.59  [3, 5, 6, 7, 9, 12, 13, 15, 16, 18, 20, 21, 22...   \n",
      "9    SibSp       1  0.27  [1, 2, 4, 10, 11, 14, 19, 26, 32, 35, 36, 40, ...   \n",
      "10   SibSp       2  0.03                                       [39, 49, 70]   \n",
      "11   SibSp       3  0.06                            [8, 25, 28, 64, 86, 89]   \n",
      "12   SibSp       4  0.03                                       [17, 51, 69]   \n",
      "13   SibSp       5  0.02                                           [60, 72]   \n",
      "14   Parch       0  0.77  [1, 2, 3, 4, 5, 6, 7, 10, 12, 13, 15, 16, 18, ...   \n",
      "15   Parch       1  0.09                [8, 11, 17, 25, 51, 55, 66, 98, 99]   \n",
      "16   Parch       2  0.11        [9, 28, 44, 59, 60, 64, 69, 72, 79, 89, 94]   \n",
      "17   Parch       3  0.01                                               [87]   \n",
      "18   Parch       5  0.02                                           [14, 26]   \n",
      "\n",
      "     Entropy  \n",
      "0   0.529361  \n",
      "1   0.529361  \n",
      "2   0.634310  \n",
      "3   0.365924  \n",
      "4   0.914926  \n",
      "5   0.701471  \n",
      "6   0.584239  \n",
      "7   0.365924  \n",
      "8   0.811278  \n",
      "9   0.529361  \n",
      "10 -0.000000  \n",
      "11  0.141441  \n",
      "12  0.080793  \n",
      "13 -0.000000  \n",
      "14  0.868721  \n",
      "15  0.242292  \n",
      "16  0.327445  \n",
      "17 -0.000000  \n",
      "18  0.080793  \n"
     ]
    }
   ],
   "source": [
    "def cal_conditional_entropy(feature, df):\n",
    "    return (df.loc[df['Feature'] == feature, 'P'] * df.loc[df['Feature'] == feature, 'Entropy']).sum()\n",
    "\n",
    "\n",
    "def calc_split_gain(feature, df):\n",
    "    split_gain = 0\n",
    "    unique_values = set(df[column].unique())\n",
    "    for value in unique_values:\n",
    "        p = df[(df[feature] == value)].shape[0] / df.shape[0]\n",
    "        split_gain -= p * np.log2(p, where=(p != 0))\n",
    "    return split_gain\n",
    "\n",
    "\n",
    "# Calc features_entropy_df DataFrame, save it to \"features_entropy.csv\"\n",
    "#   Feature   Value   P       Cases    Entropy\n",
    "# 0   Pclass    1     0.21  [2,4...     0.529361  \n",
    "# 1   Pclass    2     0.19  [10,16...   0.529361\n",
    "# ...  \n",
    "# 3   Sex      female 0.40  [2, 3, 4, 9, 10, 11, 12, 15, 16, 19, 20, 23, 2...   \n",
    "# 4   Sex      male   0.60  [1, 5, 6, 7, 8, 13, 14, 17, 18, 21, 22, 24, 27...   \n",
    "# ...\n",
    "\n",
    "features_entropy_df = pd.DataFrame(features_entropy_list, columns=['Feature', 'Value', 'P', 'Cases', 'Entropy'])\n",
    "features_entropy_df.to_csv(\"features_entropy.csv\", sep=',', index=False, encoding='utf-8')\n",
    "print(features_entropy_df)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  InfoGain  InfoSplit\n",
      "0     Sex  0.385426   0.000000\n",
      "1  Pclass  0.378621   0.472823\n",
      "2     Age  0.373942   0.000000\n",
      "3   SibSp  0.338459   0.959137\n",
      "4   Parch  0.242594   0.602997\n"
     ]
    }
   ],
   "source": [
    "# Calc gain for each column to draw a decision tree in following block\n",
    "# Conditional entropy, info gain, ratio gain \n",
    "#   Feature  InfoGain  InfoSplit\n",
    "# 0     Sex  0.385426   0.000000\n",
    "# 1  Pclass  0.378621   0.472823\n",
    "# ...\n",
    "features_by_information_gains = []\n",
    "for df_column in df.columns:\n",
    "\n",
    "    # Skip 'Survived', already calc in whole_dataset_entropy\n",
    "    if df_column == 'Survived' or df_column == 'PassengerId':\n",
    "        continue\n",
    "\n",
    "    gain = whole_dataset_entropy - cal_conditional_entropy(df_column, features_entropy_df)\n",
    "    split_gain = calc_split_gain(df_column, df)\n",
    "    features_by_information_gains.append((df_column, gain, split_gain))\n",
    "\n",
    "features_by_information_gains.sort(key=lambda x: x[1], reverse=True)\n",
    "lol = pd.DataFrame(features_by_information_gains, columns=['Feature', 'InfoGain', 'InfoSplit'])\n",
    "print(lol)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:38:12.383881Z",
     "start_time": "2024-03-23T15:38:12.244942Z"
    }
   },
   "id": "c35434a906330f55",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'False: boolean label can not be used without a boolean index'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m feature_df \u001B[38;5;241m=\u001B[39m \u001B[43mfeatures_entropy_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFeature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mfeature\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m      9\u001B[0m values \u001B[38;5;241m=\u001B[39m features_entropy_df\u001B[38;5;241m.\u001B[39mloc[features_entropy_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m feature[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValue\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_list()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m value \u001B[38;5;129;01min\u001B[39;00m values:\n",
      "File \u001B[0;32m~/src/github.com/titanic-machine-learning/venv/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1189\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[1;32m   1190\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001B[0;32m-> 1191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/src/github.com/titanic-machine-learning/venv/lib/python3.12/site-packages/pandas/core/indexing.py:1430\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1427\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;28mtuple\u001B[39m(indexer)]\n\u001B[1;32m   1429\u001B[0m \u001B[38;5;66;03m# fall thru to straight lookup\u001B[39;00m\n\u001B[0;32m-> 1430\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1431\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_label(key, axis\u001B[38;5;241m=\u001B[39maxis)\n",
      "File \u001B[0;32m~/src/github.com/titanic-machine-learning/venv/lib/python3.12/site-packages/pandas/core/indexing.py:1239\u001B[0m, in \u001B[0;36m_LocIndexer._validate_key\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1232\u001B[0m ax \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_get_axis(axis)\n\u001B[1;32m   1233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\n\u001B[1;32m   1234\u001B[0m     is_bool_dtype(ax\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m   1235\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m ax\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboolean\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1236\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ax, MultiIndex)\n\u001B[1;32m   1237\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_bool_dtype(ax\u001B[38;5;241m.\u001B[39mget_level_values(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m   1238\u001B[0m ):\n\u001B[0;32m-> 1239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[1;32m   1240\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: boolean label can not be used without a boolean index\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1241\u001B[0m     )\n\u001B[1;32m   1243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m   1244\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(key\u001B[38;5;241m.\u001B[39mstart, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key\u001B[38;5;241m.\u001B[39mstop, \u001B[38;5;28mbool\u001B[39m)\n\u001B[1;32m   1245\u001B[0m ):\n\u001B[1;32m   1246\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: boolean values can not be used in a slice\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'False: boolean label can not be used without a boolean index'"
     ]
    }
   ],
   "source": [
    "## VISUALIZE\n",
    "# Create json\n",
    "tree = Tree()\n",
    "tree.create_node(features_by_information_gains[0][0], 0)\n",
    "for index, feature in enumerate(features_by_information_gains):\n",
    "    if index == 0:\n",
    "        continue\n",
    "    feature_df = features_entropy_df.loc[[\"Feature\"] == feature[0]]\n",
    "    values = features_entropy_df.loc[features_entropy_df[\"Feature\"] == feature[0], 'Value'].to_list()\n",
    "    for value in values:\n",
    "        hashy = int(hashlib.sha256(str(value).encode('utf-8')).hexdigest(), 16)\n",
    "\n",
    "        # if all examples were survivors make \"survived\" node\n",
    "        # if all dies make \"died\" node\n",
    "        # if neither \n",
    "        tree.create_node(features_by_information_gains[index][0], hashy, parent=index - 1, data=value)\n",
    "\n",
    "with open('tree.json', 'w') as f:\n",
    "    print(tree.to_json(with_data=True), file=f)\n",
    "\n",
    "# Print decision tree using generated json\n",
    "# 4) Draw decision tree (use package) with info how many\n",
    "data = json.loads(tree.to_json())\n",
    "edges = []\n",
    "\n",
    "\n",
    "def get_edges(treedict, parent=None):\n",
    "    name = next(iter(treedict.keys()))\n",
    "    if parent is not None:\n",
    "        edges.append((parent, name))\n",
    "    for item in treedict[name][\"children\"]:\n",
    "        if isinstance(item, dict):\n",
    "            get_edges(item, parent=name)\n",
    "        else:\n",
    "            edges.append((name, item))\n",
    "\n",
    "\n",
    "get_edges(data)\n",
    "\n",
    "# Dump edge list in Graphviz DOT format\n",
    "with open('tree.dot', 'w') as f:\n",
    "    print('strict digraph tree {', file=f)\n",
    "    for row in edges:\n",
    "        print('    {0} -> {1} [ label=\"value\" ];'.format(*row), file=f)\n",
    "    print('}', file=f)\n",
    "\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# for index, feature in enumerate(features_by_gains):\n",
    "#     G.add_node(features_by_gains[index][0])\n",
    "#     feature_possible_choices = set(features_entropy_df.loc[features_entropy_df[\"Feature\"] == features_by_gains[index][0], \"Value\"])\n",
    "#     G.add_nodes_from(feature_possible_choices)\n",
    "#     for choice in feature_possible_choices:\n",
    "#         G.add_edge(features_by_gains[index][0], choice)\n",
    "# \n",
    "# pos = graphviz_layout(<G, prog=\"dot\")\n",
    "# nx.draw(G, pos, with_labels=True, font_weight='bold')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:38:16.298247Z",
     "start_time": "2024-03-23T15:38:13.296619Z"
    }
   },
   "id": "53396ea6de62e88",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7cdbf6fd25b5bacb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
